apiVersion: apps/v1
kind: Deployment
metadata:
  name: triton-inference-server
  labels:
    app: triton-inference-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: triton-inference-server
  template:
    metadata:
      labels:
        app: triton-inference-server
    spec:
      volumes:
       - name: cache
         emptyDir: {}
      containers:
        - name: triton-inference-server
          image: us-east1-docker.pkg.dev/rick-vertex-ai/triton-llm/vllama2:latest
          imagePullPolicy: IfNotPresent

          resources:
            limits:
              nvidia.com/gpu: 1
          env:
            - name: LD_PRELOAD
              value: ''
            - name: TRANSFORMERS_CACHE
              value: /home/triton-server/.cache
          args: ["tritonserver", "--model-store=gs://triton_sample_models/23_06",
                 "--strict-model-config=False",
                 "--log-verbose=1",
                 "--allow-gpu-metrics=True"]

          workingDir: /home/triton-server/
          ports:
            - containerPort: 8000
              name: http
            - containerPort: 8001
              name: grpc
            - containerPort: 8002
              name: metrics
          livenessProbe:
            httpGet:
              path: /v2/health/live
              port: http
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 60
            successThreshold: 1
            failureThreshold: 1
          readinessProbe:
            httpGet:
              path: /v2/health/ready
              port: http
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 60
            successThreshold: 1
            failureThreshold: 1

          securityContext:
            runAsUser: 1000
          volumeMounts:
            - mountPath: /home/triton-server/.cache
              name: cache